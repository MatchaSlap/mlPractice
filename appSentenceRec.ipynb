{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from janome.tokenizer import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tokenizer()\n",
    "s = 'おすすめの料理を教えてください'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "おすすめ\t名詞,サ変接続,*,*,*,*,おすすめ,オススメ,オススメ\n",
      "の\t助詞,連体化,*,*,*,*,の,ノ,ノ\n",
      "料理\t名詞,サ変接続,*,*,*,*,料理,リョウリ,リョーリ\n",
      "を\t助詞,格助詞,一般,*,*,*,を,ヲ,ヲ\n",
      "教え\t動詞,自立,*,*,一段,連用形,教える,オシエ,オシエ\n",
      "て\t助詞,接続助詞,*,*,*,*,て,テ,テ\n",
      "ください\t動詞,非自立,*,*,五段・ラ行特殊,命令ｉ,くださる,クダサイ,クダサイ\n"
     ]
    }
   ],
   "source": [
    "for token in t.tokenize(s):\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "一\n",
      "\n",
      "　うとうととして目がさめると女はいつのまにか、隣のじいさんと話を始めている。このじいさんはたしかに前の前の駅から乗ったいなか者である。発車まぎわに頓狂な声を出して駆け込んで来て、いきなり肌を\n",
      "\n",
      "\n",
      "取りかかる。与次郎だけが三四郎のそばへ来た。\n",
      "「どうだ森の女は」\n",
      "「森の女という題が悪い」\n",
      "「じゃ、なんとすればよいんだ」\n",
      "　三四郎はなんとも答えなかった。ただ口の中で迷羊、迷羊と繰り返した。\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "# ファイル読込み、内部表現化\n",
    "f = codecs.open('sanshiro.txt', \"r\", \"sjis\")\n",
    "text = f.read()\n",
    "f.close()\n",
    "\n",
    "# ファイル整形\n",
    "import re\n",
    "# ヘッダ部分の除去\n",
    "text = re.split('\\-{5,}',text)[2]\n",
    "# フッタ部分の除去\n",
    "text = re.split('底本：',text)[0]\n",
    "# | の除去\n",
    "text = text.replace('|', '')\n",
    "# ルビの削除\n",
    "text = re.sub('《.+?》', '', text)\n",
    "# 入力注の削除\n",
    "text = re.sub('［＃.+?］', '',text)\n",
    "# 空行の削除\n",
    "text = re.sub('\\n\\n', '\\n', text) \n",
    "text = re.sub('\\r', '', text)\n",
    "\n",
    "# 整形結果確認\n",
    "\n",
    "# 頭の100文字の表示 \n",
    "print(text[:100])\n",
    "# 見やすくするため、空行 \n",
    "print()\n",
    "print()\n",
    "# 後ろの100文字の表示 \n",
    "print(text[-100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "三四郎\n",
      "京都\n",
      "ちょっと\n",
      "用\n",
      "ある\n",
      "降りる\n",
      "ついで\n",
      "一\n",
      "する\n",
      "目\n",
      "さめる\n",
      "女\n",
      "隣\n",
      "じいさん\n",
      "話\n",
      "始める\n",
      "いる\n"
     ]
    }
   ],
   "source": [
    "from janome.tokenizer import Tokenizer\n",
    "# Tokenneizerインスタンスの生成 \n",
    "t = Tokenizer()\n",
    "\n",
    "# テキストを引数として、形態素解析の結果、名詞・動詞原型のみを配列で抽出する関数を定義 \n",
    "def extract_words(text):\n",
    "    tokens = t.tokenize(text)\n",
    "    return [token.base_form for token in tokens \n",
    "        if token.part_of_speech.split(',')[0] in['名詞', '動詞']]\n",
    "\n",
    "#  関数テスト\n",
    "ret = extract_words('三四郎は京都でちょっと用があって降りたついでに。')\n",
    "for word in ret:\n",
    "    print(word)\n",
    "\n",
    "# 全体のテキストを句点('。')で区切った配列にする。 \n",
    "sentences = text.split('。')\n",
    "# それぞれの文章を単語リストに変換(処理に数分かかります)\n",
    "word_list = [extract_words(sentence) for sentence in sentences]\n",
    "\n",
    "# 結果の一部を確認 \n",
    "for word in word_list[0]:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "勾配\n",
      "降下\n",
      "法\n"
     ]
    }
   ],
   "source": [
    "#  関数テスト\n",
    "ret = extract_words('勾配降下法')\n",
    "for word in ret:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vecライブラリのロード\n",
    "from gensim.models import word2vec\n",
    "\n",
    "# size: 圧縮次元数\n",
    "# min_count: 出現頻度の低いものをカットする\n",
    "# window: 前後の単語を拾う際の窓の広さを決める\n",
    "# iter: 機械学習の繰り返し回数(デフォルト:5)十分学習できていないときにこの値を調整する\n",
    "# model.wv.most_similarの結果が1に近いものばかりで、model.dict['wv']のベクトル値が小さい値ばかりの \n",
    "# ときは、学習回数が少ないと考えられます。\n",
    "# その場合、iterの値を大きくして、再度学習を行います。\n",
    "\n",
    "# 事前準備したword_listを使ってWord2Vecの学習実施\n",
    "model = word2vec.Word2Vec(word_list, size=100,min_count=5,window=5,iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.563186   -0.11887318  0.66275764 -0.29098827  0.02309809 -0.20922549\n",
      "  0.23195629  0.5226636  -0.07885285 -0.50527006 -0.66221255 -0.1025684\n",
      "  0.05737205 -1.0302899  -0.3289083   0.31853     0.12568839 -0.2614803\n",
      " -0.39552355 -0.04434584 -0.03857547 -0.88955194  0.15444139 -0.09106922\n",
      "  0.25068918  0.48266354 -0.8184265  -0.58586913  0.09914121 -0.22208445\n",
      " -0.35135096  0.0709054  -0.08293461  0.01704745 -0.4322672   0.46470514\n",
      " -0.2754094  -0.31491783  0.24328439 -0.7093739   0.01978837  0.2636638\n",
      " -0.16215883  0.6252992  -0.2530013   0.32801667 -0.00191444 -0.13582203\n",
      "  0.08644591  0.22770953  0.76143605 -0.5578901   0.60276544  0.04574628\n",
      " -0.74351424 -0.67008686 -0.8171414   0.00847305  0.21800551  0.19472255\n",
      " -0.0561062  -0.79417884  0.80249584  0.18939157 -0.33254972 -0.03130971\n",
      " -0.72211105  0.81497306 -0.29064637 -0.2791048  -0.22808504  0.02607691\n",
      "  0.4599352   0.3358488  -0.41397086  0.18989086  0.5822133   0.26180425\n",
      "  0.9677959  -0.07952777  0.42871222 -0.6222144  -0.5960752  -0.28550854\n",
      "  0.739469   -0.1690601  -0.15733077 -1.080223    0.11958857  0.35096768\n",
      "  0.37926143  0.08696495  0.42669833 -0.4619849   0.05029429 -0.10596464\n",
      " -0.23629303  0.15278427 -0.06422329  0.39814138]\n"
     ]
    }
   ],
   "source": [
    "# 結果の確認1\n",
    "# 一つ一つの単語は100次元のベクトルになっています。 \n",
    "# 「世間」のベクトル値を確認します。\n",
    "print(model.__dict__['wv']['世間'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "遊び 0.4558658003807068\n",
      "御存じ 0.45559656620025635\n",
      "あなた 0.44520139694213867\n",
      "御用 0.4353439509868622\n",
      "おいで 0.42126962542533875\n",
      "野々宮 0.419454425573349\n",
      "思える 0.4158669412136078\n",
      "めんどう 0.41556546092033386\n",
      "かた 0.4107964038848877\n",
      "おっかさん 0.4104591906070709\n"
     ]
    }
   ],
   "source": [
    "# 結果の確認2\n",
    "# 関数most_similarを使って「世間」の類似単語を調べます \n",
    "ret = model.wv.most_similar(positive=['私']) \n",
    "for item in ret:\n",
    "    print(item[0], item[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWS\t名詞,固有名詞,組織,*,*,*,*\n",
      "の\t助詞,連体化,*,*,*,*,の,ノ,ノ\n",
      "有名\t名詞,形容動詞語幹,*,*,*,*,有名,ユウメイ,ユーメイ\n",
      "な\t助動詞,*,*,*,特殊・ダ,体言接続,だ,ナ,ナ\n",
      "サービス\t名詞,サ変接続,*,*,*,*,サービス,サービス,サービス\n",
      "に\t助詞,格助詞,一般,*,*,*,に,ニ,ニ\n",
      "Amazon\t名詞,一般,*,*,*,*,*\n",
      "Elastic\t名詞,一般,*,*,*,*,*\n",
      "Compute\t名詞,一般,*,*,*,*,*\n",
      "Cloud\t名詞,一般,*,*,*,*,*\n",
      "(\t名詞,サ変接続,*,*,*,*,*\n",
      "EC\t名詞,一般,*,*,*,*,*\n",
      "2\t名詞,数,*,*,*,*,*\n",
      ")\t名詞,サ変接続,*,*,*,*,*\n",
      "と\t助詞,並立助詞,*,*,*,*,と,ト,ト\n",
      "Amazon\t名詞,固有名詞,組織,*,*,*,*\n",
      "Simple\t名詞,一般,*,*,*,*,*\n",
      "Storage\t名詞,一般,*,*,*,*,*\n",
      "Service\t名詞,一般,*,*,*,*,*\n",
      "(\t名詞,サ変接続,*,*,*,*,*\n",
      "S\t名詞,一般,*,*,*,*,*\n",
      "3\t名詞,数,*,*,*,*,*\n",
      ")\t名詞,サ変接続,*,*,*,*,*\n",
      "が\t助詞,格助詞,一般,*,*,*,が,ガ,ガ\n",
      "ある\t動詞,自立,*,*,五段・ラ行,基本形,ある,アル,アル\n",
      "。\t記号,句点,*,*,*,*,。,。,。\n",
      "これ\t名詞,代名詞,一般,*,*,*,これ,コレ,コレ\n",
      "まで\t助詞,副助詞,*,*,*,*,まで,マデ,マデ\n",
      "の\t助詞,連体化,*,*,*,*,の,ノ,ノ\n",
      "クライアント\t名詞,一般,*,*,*,*,*\n",
      "が\t助詞,格助詞,一般,*,*,*,が,ガ,ガ\n",
      "保有\t名詞,サ変接続,*,*,*,*,保有,ホユウ,ホユー\n",
      "し\t動詞,自立,*,*,サ変・スル,連用形,する,シ,シ\n",
      "て\t助詞,接続助詞,*,*,*,*,て,テ,テ\n",
      "い\t動詞,非自立,*,*,一段,連用形,いる,イ,イ\n",
      "た\t助動詞,*,*,*,特殊・タ,基本形,た,タ,タ\n",
      "物理\t名詞,一般,*,*,*,*,物理,ブツリ,ブツリ\n",
      "的\t名詞,接尾,形容動詞語幹,*,*,*,的,テキ,テキ\n",
      "な\t助動詞,*,*,*,特殊・ダ,体言接続,だ,ナ,ナ\n",
      "サーバ\t名詞,一般,*,*,*,*,サーバ,サーバ,サーバ\n",
      "ファーム\t名詞,一般,*,*,*,*,ファーム,ファーム,ファーム\n",
      "と\t助詞,格助詞,一般,*,*,*,と,ト,ト\n",
      "比較\t名詞,サ変接続,*,*,*,*,比較,ヒカク,ヒカク\n",
      "し\t動詞,自立,*,*,サ変・スル,連用形,する,シ,シ\n",
      "て\t助詞,接続助詞,*,*,*,*,て,テ,テ\n",
      "AWS\t名詞,一般,*,*,*,*,*\n",
      "は\t助詞,係助詞,*,*,*,*,は,ハ,ワ\n",
      "大\t接頭詞,名詞接続,*,*,*,*,大,ダイ,ダイ\n",
      "規模\t名詞,一般,*,*,*,*,規模,キボ,キボ\n",
      "な\t助動詞,*,*,*,特殊・ダ,体言接続,だ,ナ,ナ\n",
      "計算\t名詞,サ変接続,*,*,*,*,計算,ケイサン,ケイサン\n",
      "処理\t名詞,サ変接続,*,*,*,*,処理,ショリ,ショリ\n",
      "能力\t名詞,一般,*,*,*,*,能力,ノウリョク,ノーリョク\n",
      "を\t助詞,格助詞,一般,*,*,*,を,ヲ,ヲ\n",
      "速やか\t名詞,形容動詞語幹,*,*,*,*,速やか,スミヤカ,スミヤカ\n",
      "に\t助詞,副詞化,*,*,*,*,に,ニ,ニ\n",
      "提供\t名詞,サ変接続,*,*,*,*,提供,テイキョウ,テイキョー\n",
      "出来る\t動詞,自立,*,*,一段,基本形,出来る,デキル,デキル\n",
      "こと\t名詞,非自立,一般,*,*,*,こと,コト,コト\n",
      "が\t助詞,格助詞,一般,*,*,*,が,ガ,ガ\n",
      "強み\t名詞,一般,*,*,*,*,強み,ツヨミ,ツヨミ\n",
      "で\t助動詞,*,*,*,特殊・ダ,連用形,だ,デ,デ\n",
      "ある\t助動詞,*,*,*,五段・ラ行アル,基本形,ある,アル,アル\n",
      "。\t記号,句点,*,*,*,*,。,。,。\n",
      "EOS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import MeCab\n",
    " \n",
    "sentence = \"\"\"AWSの有名なサービスにAmazon Elastic Compute Cloud (EC2) とAmazon Simple Storage Service (S3) がある。\n",
    "これまでのクライアントが保有していた物理的なサーバファームと比較してAWSは大規模な計算処理能力を速やかに提供出来ることが強みである。\"\"\"\n",
    " \n",
    "t = MeCab.Tagger('')\n",
    "print(t.parse(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWS\t名詞,固有名詞,一般,*,*,*,AWS,アマゾンウェブサービス,アマゾンウェブサービス\n",
      "の\t助詞,連体化,*,*,*,*,の,ノ,ノ\n",
      "有名\t名詞,形容動詞語幹,*,*,*,*,有名,ユウメイ,ユーメイ\n",
      "な\t助動詞,*,*,*,特殊・ダ,体言接続,だ,ナ,ナ\n",
      "サービス\t名詞,サ変接続,*,*,*,*,サービス,サービス,サービス\n",
      "に\t助詞,格助詞,一般,*,*,*,に,ニ,ニ\n",
      "Amazon\t名詞,固有名詞,一般,*,*,*,Amazon,アマゾン,アマゾン\n",
      "Elastic\t名詞,固有名詞,一般,*,*,*,Elastic,エラスティック,エラスティック\n",
      "Compute\t名詞,一般,*,*,*,*,*\n",
      "Cloud\t名詞,一般,*,*,*,*,*\n",
      "(\t記号,一般,*,*,*,*,*\n",
      "EC2\t名詞,固有名詞,一般,*,*,*,EC2,イーシーツー,イーシーツー\n",
      ")\t記号,一般,*,*,*,*,*\n",
      "と\t助詞,並立助詞,*,*,*,*,と,ト,ト\n",
      "Amazon\t名詞,固有名詞,一般,*,*,*,Amazon,アマゾン,アマゾン\n",
      "Simple\t名詞,固有名詞,一般,*,*,*,Simple,シンプル,シンプル\n",
      "Storage\t名詞,一般,*,*,*,*,*\n",
      "Service\t名詞,一般,*,*,*,*,*\n",
      "(\t記号,一般,*,*,*,*,*\n",
      "S3\t名詞,固有名詞,人名,一般,*,*,S3,エススリーマイルスボニーアンドブレンクシナトラ,エススリーマイルスボニーアンドブレンクシナトラ\n",
      ")\t記号,一般,*,*,*,*,*\n",
      "が\t助詞,格助詞,一般,*,*,*,が,ガ,ガ\n",
      "ある\t動詞,自立,*,*,五段・ラ行,基本形,ある,アル,アル\n",
      "。\t記号,句点,*,*,*,*,。,。,。\n",
      "これ\t名詞,代名詞,一般,*,*,*,これ,コレ,コレ\n",
      "まで\t助詞,副助詞,*,*,*,*,まで,マデ,マデ\n",
      "の\t助詞,連体化,*,*,*,*,の,ノ,ノ\n",
      "クライアント\t名詞,固有名詞,一般,*,*,*,client,クライアント,クライアント\n",
      "が\t助詞,格助詞,一般,*,*,*,が,ガ,ガ\n",
      "保有\t名詞,サ変接続,*,*,*,*,保有,ホユウ,ホユー\n",
      "し\t動詞,自立,*,*,サ変・スル,連用形,する,シ,シ\n",
      "て\t助詞,接続助詞,*,*,*,*,て,テ,テ\n",
      "い\t動詞,非自立,*,*,一段,連用形,いる,イ,イ\n",
      "た\t助動詞,*,*,*,特殊・タ,基本形,た,タ,タ\n",
      "物理\t名詞,一般,*,*,*,*,物理,ブツリ,ブツリ\n",
      "的\t名詞,接尾,形容動詞語幹,*,*,*,的,テキ,テキ\n",
      "な\t助動詞,*,*,*,特殊・ダ,体言接続,だ,ナ,ナ\n",
      "サーバファーム\t名詞,固有名詞,一般,*,*,*,サーバファーム,サーバファーム,サーバファーム\n",
      "と\t助詞,格助詞,一般,*,*,*,と,ト,ト\n",
      "比較\t名詞,サ変接続,*,*,*,*,比較,ヒカク,ヒカク\n",
      "し\t動詞,自立,*,*,サ変・スル,連用形,する,シ,シ\n",
      "て\t助詞,接続助詞,*,*,*,*,て,テ,テ\n",
      "AWS\t名詞,固有名詞,一般,*,*,*,AWS,アマゾンウェブサービス,アマゾンウェブサービス\n",
      "は\t助詞,係助詞,*,*,*,*,は,ハ,ワ\n",
      "大規模\t名詞,一般,*,*,*,*,大規模,ダイキボ,ダイキボ\n",
      "な\t助動詞,*,*,*,特殊・ダ,体言接続,だ,ナ,ナ\n",
      "計算\t名詞,サ変接続,*,*,*,*,計算,ケイサン,ケイサン\n",
      "処理\t名詞,サ変接続,*,*,*,*,処理,ショリ,ショリ\n",
      "能力\t名詞,一般,*,*,*,*,能力,ノウリョク,ノーリョク\n",
      "を\t助詞,格助詞,一般,*,*,*,を,ヲ,ヲ\n",
      "速やか\t名詞,形容動詞語幹,*,*,*,*,速やか,スミヤカ,スミヤカ\n",
      "に\t助詞,副詞化,*,*,*,*,に,ニ,ニ\n",
      "提供\t名詞,サ変接続,*,*,*,*,提供,テイキョウ,テイキョー\n",
      "出来る\t動詞,自立,*,*,一段,基本形,出来る,デキル,デキル\n",
      "こと\t名詞,非自立,一般,*,*,*,こと,コト,コト\n",
      "が\t助詞,格助詞,一般,*,*,*,が,ガ,ガ\n",
      "強み\t名詞,一般,*,*,*,*,強み,ツヨミ,ツヨミ\n",
      "で\t助動詞,*,*,*,特殊・ダ,連用形,だ,デ,デ\n",
      "ある\t助動詞,*,*,*,五段・ラ行アル,基本形,ある,アル,アル\n",
      "。\t記号,句点,*,*,*,*,。,。,。\n",
      "EOS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import MeCab\n",
    " \n",
    "sentence = \"\"\"AWSの有名なサービスにAmazon Elastic Compute Cloud (EC2) とAmazon Simple Storage Service (S3) がある。\n",
    "これまでのクライアントが保有していた物理的なサーバファームと比較してAWSは大規模な計算処理能力を速やかに提供出来ることが強みである。\"\"\"\n",
    " \n",
    "t = MeCab.Tagger('-d /usr/local/lib/mecab/dic/mecab-ipadic-neologd')\n",
    "print(t.parse(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "def ExtractText(sentence):\n",
    "    t = MeCab.Tagger('-d /usr/local/lib/mecab/dic/mecab-ipadic-neologd')\n",
    "    s = preprocessing(sentence)\n",
    "    out = []\n",
    "    for chunk in t.parse(s).splitlines()[:-1]:\n",
    "        (surface,feature) = chunk.split('\\t')\n",
    "        out.append(surface)\n",
    "#         (surface, feature) = chunk.split('\\t')\n",
    "#         if feature.startswith('名詞'):\n",
    "#             nouns.append(surface)\n",
    "    return out\n",
    "\n",
    "def preprocessing(sentence):\n",
    "    return sentence.rstrip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['席',\n",
       " 'を',\n",
       " '外さ',\n",
       " 'れる',\n",
       " '場合',\n",
       " 'は',\n",
       " '１',\n",
       " '時間',\n",
       " '以内',\n",
       " 'に',\n",
       " '戻っ',\n",
       " 'て',\n",
       " '来',\n",
       " 'て',\n",
       " 'いただけ',\n",
       " 'ます',\n",
       " 'よう',\n",
       " '、',\n",
       " 'よろしく',\n",
       " 'お願い',\n",
       " 'いたし',\n",
       " 'ます',\n",
       " '。']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(ExtractText(\"私の名前はトーマスです。\"))\n",
    "nodes = ExtractText(\"席を外される場合は１時間以内に戻って来ていただけますよう、よろしくお願いいたします。\")\n",
    "nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "私\t名詞,代名詞,一般,*,*,*,私,ワタシ,ワタシ\n",
      "の\t助詞,連体化,*,*,*,*,の,ノ,ノ\n",
      "名前\t名詞,一般,*,*,*,*,名前,ナマエ,ナマエ\n",
      "は\t助詞,係助詞,*,*,*,*,は,ハ,ワ\n",
      "トーマス\t名詞,固有名詞,人名,名,*,*,トーマス,トーマス,トーマス\n",
      "です\t助動詞,*,*,*,特殊・デス,基本形,です,デス,デス\n",
      "。\t記号,句点,*,*,*,*,。,。,。\n",
      "EOS\n"
     ]
    }
   ],
   "source": [
    "print(nodes.rstrip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "私\t名詞,代名詞,一般,*,*,*,私,ワタシ,ワタシ\n",
      "の\t助詞,連体化,*,*,*,*,の,ノ,ノ\n",
      "名前\t名詞,一般,*,*,*,*,名前,ナマエ,ナマエ\n",
      "は\t助詞,係助詞,*,*,*,*,は,ハ,ワ\n",
      "トーマス\t名詞,固有名詞,人名,名,*,*,トーマス,トーマス,トーマス\n",
      "です\t助動詞,*,*,*,特殊・デス,基本形,です,デス,デス\n",
      "。\t記号,句点,*,*,*,*,。,。,。\n",
      "EOS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BiLSTM + CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_and_labels(filename):\n",
    "    sents, labels = [], []\n",
    "    words, tags = [], []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip()\n",
    "            if line:\n",
    "                word, tag = line.split('\\t')\n",
    "                words.append(word)\n",
    "                tags.append(tag)\n",
    "            else:\n",
    "                sents.append(words)\n",
    "                labels.append(tags)\n",
    "                words, tags = [], []\n",
    "                \n",
    "    return sents, labels\n",
    "\n",
    "train_file = './resource/train.txt'\n",
    "valid_file = './resource/valid.txt'\n",
    "\n",
    "x_train, y_train = load_data_and_labels(train_file)\n",
    "x_valid, y_valid = load_data_and_labels(valid_file)\n",
    "# x_train[0]\n",
    "# ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK = '<UNK>'\n",
    "PAD = '<PAD>'\n",
    "\n",
    "vocab_word = {PAD: 0, UNK: 1}\n",
    "vocab_char = {PAD: 0, UNK: 1}\n",
    "vocab_label = {PAD: 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in x_train:\n",
    "    for w in sent:\n",
    "        # create char dictionary.\n",
    "        for c in w:\n",
    "            if c in vocab_char:\n",
    "                continue\n",
    "            vocab_char[c] = len(vocab_char)\n",
    "\n",
    "        # create word dictionary.\n",
    "        if w in vocab_word:\n",
    "            continue\n",
    "        vocab_word[w] = len(vocab_word)\n",
    "\n",
    "# create label dictionary.\n",
    "for labels in y_train:\n",
    "    for tag in labels:\n",
    "        if tag in vocab_label:\n",
    "            continue\n",
    "        vocab_label[tag] = len(vocab_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23625"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## keras-example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/20\n",
      "54000/54000 [==============================] - 1s 27us/step - loss: 2.1078 - acc: 0.4225 - val_loss: 1.8763 - val_acc: 0.6630\n",
      "Epoch 2/20\n",
      "54000/54000 [==============================] - 1s 25us/step - loss: 1.7185 - acc: 0.6839 - val_loss: 1.5149 - val_acc: 0.7690\n",
      "Epoch 3/20\n",
      "54000/54000 [==============================] - 1s 25us/step - loss: 1.4124 - acc: 0.7513 - val_loss: 1.2357 - val_acc: 0.8170\n",
      "Epoch 4/20\n",
      "54000/54000 [==============================] - 1s 26us/step - loss: 1.1826 - acc: 0.7872 - val_loss: 1.0299 - val_acc: 0.8413\n",
      "Epoch 5/20\n",
      "54000/54000 [==============================] - 1s 26us/step - loss: 1.0158 - acc: 0.8096 - val_loss: 0.8834 - val_acc: 0.8568\n",
      "Epoch 6/20\n",
      "54000/54000 [==============================] - 1s 25us/step - loss: 0.8952 - acc: 0.8242 - val_loss: 0.7773 - val_acc: 0.8708\n",
      "Epoch 7/20\n",
      "54000/54000 [==============================] - 1s 25us/step - loss: 0.8064 - acc: 0.8344 - val_loss: 0.6958 - val_acc: 0.8788\n",
      "Epoch 8/20\n",
      "54000/54000 [==============================] - 1s 24us/step - loss: 0.7391 - acc: 0.8420 - val_loss: 0.6356 - val_acc: 0.8803\n",
      "Epoch 9/20\n",
      "54000/54000 [==============================] - 1s 24us/step - loss: 0.6867 - acc: 0.8488 - val_loss: 0.5877 - val_acc: 0.8832\n",
      "Epoch 10/20\n",
      "54000/54000 [==============================] - 1s 24us/step - loss: 0.6450 - acc: 0.8528 - val_loss: 0.5493 - val_acc: 0.8915\n",
      "Epoch 11/20\n",
      "54000/54000 [==============================] - 1s 24us/step - loss: 0.6110 - acc: 0.8584 - val_loss: 0.5182 - val_acc: 0.8917\n",
      "Epoch 12/20\n",
      "54000/54000 [==============================] - 1s 24us/step - loss: 0.5826 - acc: 0.8619 - val_loss: 0.4931 - val_acc: 0.8940\n",
      "Epoch 13/20\n",
      "54000/54000 [==============================] - 1s 24us/step - loss: 0.5588 - acc: 0.8657 - val_loss: 0.4704 - val_acc: 0.8967\n",
      "Epoch 14/20\n",
      "54000/54000 [==============================] - 1s 24us/step - loss: 0.5385 - acc: 0.8684 - val_loss: 0.4519 - val_acc: 0.9000\n",
      "Epoch 15/20\n",
      "54000/54000 [==============================] - 1s 24us/step - loss: 0.5209 - acc: 0.8711 - val_loss: 0.4358 - val_acc: 0.9023\n",
      "Epoch 16/20\n",
      "54000/54000 [==============================] - 1s 24us/step - loss: 0.5056 - acc: 0.8733 - val_loss: 0.4219 - val_acc: 0.9020\n",
      "Epoch 17/20\n",
      "54000/54000 [==============================] - 1s 24us/step - loss: 0.4922 - acc: 0.8752 - val_loss: 0.4097 - val_acc: 0.9037\n",
      "Epoch 18/20\n",
      "54000/54000 [==============================] - 1s 25us/step - loss: 0.4802 - acc: 0.8773 - val_loss: 0.3987 - val_acc: 0.9035\n",
      "Epoch 19/20\n",
      "54000/54000 [==============================] - 1s 25us/step - loss: 0.4695 - acc: 0.8791 - val_loss: 0.3892 - val_acc: 0.9055\n",
      "Epoch 20/20\n",
      "54000/54000 [==============================] - 1s 24us/step - loss: 0.4599 - acc: 0.8801 - val_loss: 0.3805 - val_acc: 0.9067\n",
      "10000/10000 [==============================] - 0s 32us/step\n",
      "test accuracy :  0.8901\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.utils import np_utils\n",
    " \n",
    "# Kerasに含まれるMNISTデータの取得\n",
    "# 初回はダウンロードが発生するため時間がかかる\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    " \n",
    "# 配列の整形と、色の範囲を0-255 -> 0-1に変換\n",
    "X_train = X_train.reshape(60000, 784) / 255\n",
    "X_test = X_test.reshape(10000, 784) / 255\n",
    " \n",
    "# 正解データを数値からダミー変数の形式に変換\n",
    "# これは例えば0, 1, 2の3値の分類の正解ラベル5件のデータが以下のような配列になってるとして\n",
    "#   [0, 1, 2, 1, 0]\n",
    "# 以下のような形式に変換する\n",
    "#   [[1, 0, 0],\n",
    "#    [0, 1, 0],\n",
    "#    [0, 0, 1],\n",
    "#    [0, 1, 0],\n",
    "#    [1, 0, 0]]\n",
    "# 列方向が0, 1, 2、行方向が各データに対応し、元のデータで正解となる部分が1、それ以外が0となるように展開してる\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    " \n",
    "# ネットワークの定義\n",
    "# 各層や活性関数に該当するレイヤを順に入れていく\n",
    "# 作成したあとにmodel.add()で追加することも可能\n",
    "model = Sequential([\n",
    "        Dense(512, input_shape=(784,)),\n",
    "        Activation('sigmoid'),\n",
    "        Dense(10),\n",
    "        Activation('softmax')\n",
    "    ])\n",
    "# 損失関数、 最適化アルゴリズムなどを設定しモデルのコンパイルを行う\n",
    "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    " \n",
    "# 学習処理の実行\n",
    "model.fit(X_train, y_train, batch_size=200, verbose=1, epochs=20, validation_split=0.1)\n",
    " \n",
    "# 予測\n",
    "score = model.evaluate(X_test, y_test, verbose=1)\n",
    "print('test accuracy : ', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Embedding, Input\n",
    "from keras.models import Model\n",
    "\n",
    "from crf import CRFLayer\n",
    "\n",
    "# Hyperparameter settings.\n",
    "vocab_size = 20\n",
    "n_classes = 11\n",
    "batch_size = 2\n",
    "maxlen = 2\n",
    "\n",
    "# Random features.\n",
    "x = np.random.randint(1, vocab_size, size=(batch_size, maxlen))\n",
    "\n",
    "# Random tag indices representing the gold sequence.\n",
    "y = np.random.randint(n_classes, size=(batch_size, maxlen))\n",
    "y = np.eye(n_classes)[y]\n",
    "\n",
    "# All sequences in this example have the same length, but they can be variable in a real model.\n",
    "s = np.asarray([maxlen] * batch_size, dtype='int32')\n",
    "\n",
    "# Build an example model.\n",
    "word_ids = Input(batch_shape=(batch_size, maxlen), dtype='int32')\n",
    "sequence_lengths = Input(batch_shape=[batch_size, 1], dtype='int32')\n",
    "\n",
    "word_embeddings = Embedding(vocab_size, n_classes)(word_ids)\n",
    "crf = CRFLayer()\n",
    "pred = crf(inputs=[word_embeddings, sequence_lengths])\n",
    "model = Model(inputs=[word_ids, sequence_lengths], outputs=[pred])\n",
    "model.compile(loss=crf.loss, optimizer='sgd')\n",
    "\n",
    "# Train first 1 batch.\n",
    "model.train_on_batch([x, s], y)\n",
    "\n",
    "# Save the model\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "[[16 12]\n",
      " [18 13]]\n",
      "y1\n",
      "[[7 5]\n",
      " [1 5]]\n",
      "y2\n",
      "[[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
      "\n",
      " [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]]\n",
      "s\n",
      "[2 2]\n"
     ]
    }
   ],
   "source": [
    "# Random features.\n",
    "x = np.random.randint(1, vocab_size, size=(batch_size, maxlen))\n",
    "print(\"x\")\n",
    "print(x)\n",
    "\n",
    "# Random tag indices representing the gold sequence.\n",
    "y = np.random.randint(n_classes, size=(batch_size, maxlen))\n",
    "\n",
    "print(\"y1\")\n",
    "print(y)\n",
    "\n",
    "y = np.eye(n_classes)[y]\n",
    "print(\"y2\")\n",
    "print(y)\n",
    "\n",
    "# All sequences in this example have the same length, but they can be variable in a real model.\n",
    "s = np.asarray([maxlen] * batch_size, dtype='int32')\n",
    "print(\"s\")\n",
    "print(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## keras-practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ダミーデータ作成\n",
    "x_train = np.random.random((1000,20))\n",
    "y_train = keras.utils.to_categorical(np.random.randint(10, size=(1000, 1)), num_classes=10)\n",
    "x_test = np.random.random((100, 20))\n",
    "y_test = keras.utils.to_categorical(np.random.randint(10, size=(100,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=20))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1000/1000 [==============================] - 0s 167us/step - loss: 2.4034 - acc: 0.0910\n",
      "Epoch 2/20\n",
      "1000/1000 [==============================] - 0s 12us/step - loss: 2.3835 - acc: 0.1030\n",
      "Epoch 3/20\n",
      "1000/1000 [==============================] - 0s 14us/step - loss: 2.3464 - acc: 0.1050\n",
      "Epoch 4/20\n",
      "1000/1000 [==============================] - 0s 14us/step - loss: 2.3439 - acc: 0.0950\n",
      "Epoch 5/20\n",
      "1000/1000 [==============================] - 0s 16us/step - loss: 2.3195 - acc: 0.1090\n",
      "Epoch 6/20\n",
      "1000/1000 [==============================] - 0s 16us/step - loss: 2.3268 - acc: 0.0940\n",
      "Epoch 7/20\n",
      "1000/1000 [==============================] - 0s 17us/step - loss: 2.3173 - acc: 0.0900\n",
      "Epoch 8/20\n",
      "1000/1000 [==============================] - 0s 18us/step - loss: 2.3057 - acc: 0.1090\n",
      "Epoch 9/20\n",
      "1000/1000 [==============================] - 0s 17us/step - loss: 2.3131 - acc: 0.1100\n",
      "Epoch 10/20\n",
      "1000/1000 [==============================] - 0s 15us/step - loss: 2.3012 - acc: 0.1150\n",
      "Epoch 11/20\n",
      "1000/1000 [==============================] - 0s 17us/step - loss: 2.3044 - acc: 0.1110\n",
      "Epoch 12/20\n",
      "1000/1000 [==============================] - 0s 14us/step - loss: 2.3022 - acc: 0.1220\n",
      "Epoch 13/20\n",
      "1000/1000 [==============================] - 0s 15us/step - loss: 2.3045 - acc: 0.1170\n",
      "Epoch 14/20\n",
      "1000/1000 [==============================] - 0s 14us/step - loss: 2.3060 - acc: 0.1120\n",
      "Epoch 15/20\n",
      "1000/1000 [==============================] - 0s 15us/step - loss: 2.2999 - acc: 0.1210\n",
      "Epoch 16/20\n",
      "1000/1000 [==============================] - 0s 16us/step - loss: 2.2921 - acc: 0.1260\n",
      "Epoch 17/20\n",
      "1000/1000 [==============================] - 0s 12us/step - loss: 2.3003 - acc: 0.1180\n",
      "Epoch 18/20\n",
      "1000/1000 [==============================] - 0s 17us/step - loss: 2.2920 - acc: 0.1340\n",
      "Epoch 19/20\n",
      "1000/1000 [==============================] - 0s 14us/step - loss: 2.2952 - acc: 0.1320\n",
      "Epoch 20/20\n",
      "1000/1000 [==============================] - 0s 15us/step - loss: 2.2980 - acc: 0.1300\n",
      "100/100 [==============================] - 0s 350us/step\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=20, batch_size=128)\n",
    "score = model.evaluate(x_test, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
